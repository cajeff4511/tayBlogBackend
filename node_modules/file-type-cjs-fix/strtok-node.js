module.exports = require('./strtok')

const { promises: fs } = require('fs')
const AbstractTokenizer = require('./tokenizer')

class FileTokenizer extends AbstractTokenizer {
  constructor(fd, fileInfo) {
    super(fileInfo)
    this.fd = fd
  }
  async readBuffer(uint8Array, options) {
    const normOptions = this.normalizeOptions(uint8Array, options)
    this.position = normOptions.position
    const res = await fs.read(this.fd, uint8Array, normOptions.offset, normOptions.length, normOptions.position)
    this.position += res.bytesRead
    if (res.bytesRead < normOptions.length && (!options || !options.mayBeLess)) {
      throw new EndOfStreamError()
    }
    return res.bytesRead
  }
  async peekBuffer(uint8Array, options) {
    const normOptions = this.normalizeOptions(uint8Array, options)
    const res = await fs.read(this.fd, uint8Array, normOptions.offset, normOptions.length, normOptions.position)
    if ((!normOptions.mayBeLess) && res.bytesRead < normOptions.length) {
      throw new EndOfStreamError()
    }
    return res.bytesRead
  }
  async close() {
    return fs.close(this.fd)
  }
}

exports.fromStream = async (stream, fileInfo) => {
  fileInfo = fileInfo ? fileInfo : {}
  if (stream.path) {
    const stat = await fs.stat(stream.path)
    fileInfo.path = stream.path
    fileInfo.size = stat.size
  }
  return new exports.ReadStreamTokenizer(stream, fileInfo)
}

exports.fromFile = async (sourceFilePath) => {
  const stat = await fs.stat(sourceFilePath)
  if (!stat.isFile)
    throw new Error(`File not a file: ${sourceFilePath}`)
  const fd = await fs.open(sourceFilePath, 'r')
  return new FileTokenizer(fd, { path: sourceFilePath, size: stat.size })
}
